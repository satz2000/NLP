{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockSentimentAnalysis-ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgBOma8ObJ-K",
        "outputId": "ac51c873-d6a9-46de-e5dc-d2c555ae6bb8"
      },
      "source": [
        "# importing dataset\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet'])    # downloading for word tokenization\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# evaluating model using metrices\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "4cp7rLmDbOiz",
        "outputId": "943bff2a-48bc-46bd-f185-1d6398e12eba"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Deep Learning/Data.csv\", encoding = \"ISO-8859-1\")\n",
        "df.head(2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0</td>\n",
              "      <td>A 'hindrance to operations': extracts from the...</td>\n",
              "      <td>Scorecard</td>\n",
              "      <td>Hughes' instant hit buoys Blues</td>\n",
              "      <td>Jack gets his skates on at ice-cold Alex</td>\n",
              "      <td>Chaos as Maracana builds up for United</td>\n",
              "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
              "      <td>Hungry Spurs sense rich pickings</td>\n",
              "      <td>Gunners so wide of an easy target</td>\n",
              "      <td>Derby raise a glass to Strupar's debut double</td>\n",
              "      <td>Southgate strikes, Leeds pay the penalty</td>\n",
              "      <td>Hammers hand Robson a youthful lesson</td>\n",
              "      <td>Saints party like it's 1999</td>\n",
              "      <td>Wear wolves have turned into lambs</td>\n",
              "      <td>Stump mike catches testy Gough's taunt</td>\n",
              "      <td>Langer escapes to hit 167</td>\n",
              "      <td>Flintoff injury piles on woe for England</td>\n",
              "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
              "      <td>Kohl's successor drawn into scandal</td>\n",
              "      <td>The difference between men and women</td>\n",
              "      <td>Sara Denver, nurse turned solicitor</td>\n",
              "      <td>Diana's landmine crusade put Tories in a panic</td>\n",
              "      <td>Yeltsin's resignation caught opposition flat-f...</td>\n",
              "      <td>Russian roulette</td>\n",
              "      <td>Sold out</td>\n",
              "      <td>Recovering a title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0</td>\n",
              "      <td>Scorecard</td>\n",
              "      <td>The best lake scene</td>\n",
              "      <td>Leader: German sleaze inquiry</td>\n",
              "      <td>Cheerio, boyo</td>\n",
              "      <td>The main recommendations</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>Hopkins 'furious' at Foster's lack of Hannibal...</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>A tale of two tails</td>\n",
              "      <td>I say what I like and I like what I say</td>\n",
              "      <td>Elbows, Eyes and Nipples</td>\n",
              "      <td>Task force to assess risk of asteroid collision</td>\n",
              "      <td>How I found myself at last</td>\n",
              "      <td>On the critical list</td>\n",
              "      <td>The timing of their lives</td>\n",
              "      <td>Dear doctor</td>\n",
              "      <td>Irish court halts IRA man's extradition to Nor...</td>\n",
              "      <td>Burundi peace initiative fades after rebels re...</td>\n",
              "      <td>PE points the way forward to the ECB</td>\n",
              "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
              "      <td>Jane Ratcliffe</td>\n",
              "      <td>Yet more things you wouldn't know without the ...</td>\n",
              "      <td>Millennium bug fails to bite</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                         Top25\n",
              "0  2000-01-03  ...            Recovering a title\n",
              "1  2000-01-04  ...  Millennium bug fails to bite\n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHBDwzAPbPX2"
      },
      "source": [
        "train = df[df['Date'] < '20150101']\n",
        "test = df[df['Date'] > '20141231']\n",
        "y_train = train['Label']\n",
        "y_test = test['Label']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "_KlRRSZzbd_j",
        "outputId": "1a85cf91-d3ce-49c2-8b38-26e6a0b6498a"
      },
      "source": [
        "data = train.iloc[:, 2:].copy()\n",
        "# to remove the unwanted variable like (comma colen etc)\n",
        "data.replace(\"[^a-zA-Z0-9]\", \" \", regex=True, inplace=True)\n",
        "data.head(2)   # everything is cleaned"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A  hindrance to operations   extracts from the...</td>\n",
              "      <td>Scorecard</td>\n",
              "      <td>Hughes  instant hit buoys Blues</td>\n",
              "      <td>Jack gets his skates on at ice cold Alex</td>\n",
              "      <td>Chaos as Maracana builds up for United</td>\n",
              "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
              "      <td>Hungry Spurs sense rich pickings</td>\n",
              "      <td>Gunners so wide of an easy target</td>\n",
              "      <td>Derby raise a glass to Strupar s debut double</td>\n",
              "      <td>Southgate strikes  Leeds pay the penalty</td>\n",
              "      <td>Hammers hand Robson a youthful lesson</td>\n",
              "      <td>Saints party like it s 1999</td>\n",
              "      <td>Wear wolves have turned into lambs</td>\n",
              "      <td>Stump mike catches testy Gough s taunt</td>\n",
              "      <td>Langer escapes to hit 167</td>\n",
              "      <td>Flintoff injury piles on woe for England</td>\n",
              "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
              "      <td>Kohl s successor drawn into scandal</td>\n",
              "      <td>The difference between men and women</td>\n",
              "      <td>Sara Denver  nurse turned solicitor</td>\n",
              "      <td>Diana s landmine crusade put Tories in a panic</td>\n",
              "      <td>Yeltsin s resignation caught opposition flat f...</td>\n",
              "      <td>Russian roulette</td>\n",
              "      <td>Sold out</td>\n",
              "      <td>Recovering a title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Scorecard</td>\n",
              "      <td>The best lake scene</td>\n",
              "      <td>Leader  German sleaze inquiry</td>\n",
              "      <td>Cheerio  boyo</td>\n",
              "      <td>The main recommendations</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>Hopkins  furious  at Foster s lack of Hannibal...</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>A tale of two tails</td>\n",
              "      <td>I say what I like and I like what I say</td>\n",
              "      <td>Elbows  Eyes and Nipples</td>\n",
              "      <td>Task force to assess risk of asteroid collision</td>\n",
              "      <td>How I found myself at last</td>\n",
              "      <td>On the critical list</td>\n",
              "      <td>The timing of their lives</td>\n",
              "      <td>Dear doctor</td>\n",
              "      <td>Irish court halts IRA man s extradition to Nor...</td>\n",
              "      <td>Burundi peace initiative fades after rebels re...</td>\n",
              "      <td>PE points the way forward to the ECB</td>\n",
              "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
              "      <td>Jane Ratcliffe</td>\n",
              "      <td>Yet more things you wouldn t know without the ...</td>\n",
              "      <td>Millennium bug fails to bite</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Top1  ...                         Top25\n",
              "0  A  hindrance to operations   extracts from the...  ...            Recovering a title\n",
              "1                                          Scorecard  ...  Millennium bug fails to bite\n",
              "\n",
              "[2 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Qg3pj7gMbzxq",
        "outputId": "2f05c15f-98ce-4726-ceb6-aa5eb247dceb"
      },
      "source": [
        "# now convert text data into lowercase\n",
        "for feature in data.columns:\n",
        "  data[feature] = data[feature].str.lower()\n",
        "data.head(1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a  hindrance to operations   extracts from the...</td>\n",
              "      <td>scorecard</td>\n",
              "      <td>hughes  instant hit buoys blues</td>\n",
              "      <td>jack gets his skates on at ice cold alex</td>\n",
              "      <td>chaos as maracana builds up for united</td>\n",
              "      <td>depleted leicester prevail as elliott spoils e...</td>\n",
              "      <td>hungry spurs sense rich pickings</td>\n",
              "      <td>gunners so wide of an easy target</td>\n",
              "      <td>derby raise a glass to strupar s debut double</td>\n",
              "      <td>southgate strikes  leeds pay the penalty</td>\n",
              "      <td>hammers hand robson a youthful lesson</td>\n",
              "      <td>saints party like it s 1999</td>\n",
              "      <td>wear wolves have turned into lambs</td>\n",
              "      <td>stump mike catches testy gough s taunt</td>\n",
              "      <td>langer escapes to hit 167</td>\n",
              "      <td>flintoff injury piles on woe for england</td>\n",
              "      <td>hunters threaten jospin with new battle of the...</td>\n",
              "      <td>kohl s successor drawn into scandal</td>\n",
              "      <td>the difference between men and women</td>\n",
              "      <td>sara denver  nurse turned solicitor</td>\n",
              "      <td>diana s landmine crusade put tories in a panic</td>\n",
              "      <td>yeltsin s resignation caught opposition flat f...</td>\n",
              "      <td>russian roulette</td>\n",
              "      <td>sold out</td>\n",
              "      <td>recovering a title</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Top1  ...               Top25\n",
              "0  a  hindrance to operations   extracts from the...  ...  recovering a title\n",
              "\n",
              "[1 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gv_Z2SPcPpt"
      },
      "source": [
        "X_data = []\n",
        "for i in range(len(data.index)):\n",
        "  X_data.append(\" \".join(str(feature) for feature in data.iloc[i, 0:]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mYIymmrdzOY"
      },
      "source": [
        "# converts para into individual words \n",
        "stemming = PorterStemmer()\n",
        "X = []\n",
        "for i in range(len(X_data)):\n",
        "  words = nltk.word_tokenize(X_data[i])\n",
        "  word = [stemming.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  word = \" \".join(word)\n",
        "  X.append(word)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nl-syMUcdga"
      },
      "source": [
        "# Convert text into vectors using TF-IDF\n",
        "vector = CountVectorizer(max_features=5000, ngram_range=(2,2))\n",
        "X_train = vector.fit_transform(X)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_TSrn0Ocq5Q",
        "outputId": "944b112a-d536-442e-c95a-c365010d8118"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3975, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXE5_GGsctiV"
      },
      "source": [
        "randomclassifier=RandomForestClassifier(n_estimators=300,criterion='entropy', )\n",
        "rfc = randomclassifier.fit(X_train, y_train)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbqcmTWjc5ng"
      },
      "source": [
        "## Predict for the Test Dataset\n",
        "data = test.iloc[:, 2:].copy()\n",
        "\n",
        "data.replace(\"[^a-zA-Z0-9]\", \" \", regex=True, inplace=True)\n",
        "\n",
        "for feature in data.columns:\n",
        "  data[feature] = data[feature].str.lower()\n",
        "\n",
        "X_test = []\n",
        "for i in range(len(data.index)):\n",
        "  X_test.append(' '.join(str(x) for x in data.iloc[i,0:]))\n",
        "\n",
        "stemming = PorterStemmer()\n",
        "y = []\n",
        "for i in range(len(X_test)):\n",
        "  words = nltk.word_tokenize(X_test[i])\n",
        "  word = [stemming.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "  word = \" \".join(word)\n",
        "  y.append(word)\n",
        "\n",
        "X_test = vector.transform(y)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_loDGg5km3f"
      },
      "source": [
        "# prediction\n",
        "pred = randomclassifier.predict(X_test)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIOadH2MdXlp",
        "outputId": "5aecacb7-e19b-4eac-a0fc-5fd3e4221e69"
      },
      "source": [
        "print(f\"Confusion matrix :\\n {confusion_matrix(y_test, pred)}\")\n",
        "print(f\"Accuracy score : {accuracy_score(y_test, pred)}\")\n",
        "print(f\"Classification report :\\n {classification_report(y_test, pred)}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix :\n",
            " [[156  30]\n",
            " [ 28 164]]\n",
            "Accuracy score : 0.8465608465608465\n",
            "Classification report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       186\n",
            "           1       0.85      0.85      0.85       192\n",
            "\n",
            "    accuracy                           0.85       378\n",
            "   macro avg       0.85      0.85      0.85       378\n",
            "weighted avg       0.85      0.85      0.85       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBBpkzpQiRtF"
      },
      "source": [
        "naive_baiye = MultinomialNB(alpha = 0.8)\n",
        "model = naive_baiye.fit(X_train, y_train)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyhGbT9JinFe"
      },
      "source": [
        "pred = naive_baiye.predict(X_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DwoUmWjiq5S",
        "outputId": "9975a054-b41f-46d1-a69c-cb5282e0cdf2"
      },
      "source": [
        "print(f\"Confusion matrix :\\n {confusion_matrix(y_test, pred)}\")\n",
        "print(f\"Accuracy score : {accuracy_score(y_test, pred)}\")\n",
        "print(f\"Classification report :\\n {classification_report(y_test, pred)}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix :\n",
            " [[156  30]\n",
            " [ 39 153]]\n",
            "Accuracy score : 0.8174603174603174\n",
            "Classification report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82       186\n",
            "           1       0.84      0.80      0.82       192\n",
            "\n",
            "    accuracy                           0.82       378\n",
            "   macro avg       0.82      0.82      0.82       378\n",
            "weighted avg       0.82      0.82      0.82       378\n",
            "\n"
          ]
        }
      ]
    }
  ]
}